{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer-wikitext2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3BBxqe1XhUMrf+nXxaSJD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6vd3Lbl758-U","executionInfo":{"status":"ok","timestamp":1616000422924,"user_tz":240,"elapsed":628,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["%%capture\n","!git clone https://github.com/ronakdm/input-marginalization.git"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sves2bjn6LOb","executionInfo":{"status":"ok","timestamp":1616000423477,"user_tz":240,"elapsed":1018,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"23569a30-c4aa-40b9-da59-e700734b7d36"},"source":["%%bash\n","cd input-marginalization\n","git pull\n","cd .."],"execution_count":12,"outputs":[{"output_type":"stream","text":["Updating 37d0591..b24eb82\n","Fast-forward\n"," generative_model/data/wikitext-2/test.txt    |  4358 ---\n"," generative_model/data/wikitext-2/train.txt   | 36718 -------------------------\n"," generative_model/data/wikitext-2/valid.txt   |  3760 ---\n"," generative_model/dataset.py                  |    66 -\n"," generative_model/test.py                     |    58 -\n"," generative_model/transformer-wikitext2.ipynb |     2 +-\n"," 6 files changed, 1 insertion(+), 44961 deletions(-)\n"," delete mode 100644 generative_model/data/wikitext-2/test.txt\n"," delete mode 100644 generative_model/data/wikitext-2/train.txt\n"," delete mode 100644 generative_model/data/wikitext-2/valid.txt\n"," delete mode 100644 generative_model/dataset.py\n"," delete mode 100644 generative_model/test.py\n"],"name":"stdout"},{"output_type":"stream","text":["From https://github.com/ronakdm/input-marginalization\n","   37d0591..b24eb82  main       -> origin/main\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCpWeaBF6Q3Z","executionInfo":{"status":"ok","timestamp":1616000425831,"user_tz":240,"elapsed":484,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"351204cb-b586-453f-a324-d22d422808cf"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","save_dir = \"/content/gdrive/My Drive/input-marginalization\""],"execution_count":13,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8b07E86BDZVJ","executionInfo":{"status":"ok","timestamp":1616000428477,"user_tz":240,"elapsed":2585,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["%%capture\n","!pip install transformers"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"9aqNpGzY6Svi","executionInfo":{"status":"ok","timestamp":1616000428478,"user_tz":240,"elapsed":2023,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["import os,sys,time,math,textwrap\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","sys.path.append(\"input-marginalization/generative_model\")\n","# import dataset, transformer\n","import transformer\n","from gen_model_utils import WikiText2Dataset\n","\n","# root = 'input-marginalization/generative_model/data/wikitext-2'\n","root = 'input-marginalization/generative_model/data/wikitext-2-raw'"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWODCRsb6veI","executionInfo":{"status":"ok","timestamp":1616000429596,"user_tz":240,"elapsed":287,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["lr = .00035\n","context = 150\n","batch_size = 32\n","log_interval = 50\n","\n","heads = 1\n","depth = 1\n","\n","torch.manual_seed(0)\n","device = torch.device(\"cuda\")"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFANQMs367hd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616000479137,"user_tz":240,"elapsed":49176,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"765cecca-5d30-45bd-a577-3637d9c0cae5"},"source":["# train_data = dataset.WikiText2(root, context, dataset.DatasetSplit.train)\n","# valid_data = dataset.WikiText2(root, context, dataset.DatasetSplit.valid)\n","# test_data = dataset.WikiText2(root, context, dataset.DatasetSplit.test)\n","\n","train_data = WikiText2Dataset(file_path=root+\"/wiki.train.raw\", seq_len=context)\n","valid_data = WikiText2Dataset(file_path=root+\"/wiki.valid.raw\", seq_len=context)\n","test_data = WikiText2Dataset(file_path=root+\"/wiki.test.raw\", seq_len=context)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (2303697 > 512). Running this sequence through the model will result in indexing errors\n","Token indices sequence length is longer than the specified maximum sequence length for this model (238658 > 512). Running this sequence through the model will result in indexing errors\n","Token indices sequence length is longer than the specified maximum sequence length for this model (273180 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vScmsN496-F6","executionInfo":{"status":"ok","timestamp":1616000479399,"user_tz":240,"elapsed":245,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["def evaluate(data):\n","    model.eval()\n","    with torch.no_grad():\n","        loss = 0.\n","        loader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size,shuffle=False)\n","        for i, (x,y) in enumerate(loader):\n","            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n","            yhat = model(x).view(-1, train_data.word_count())\n","            loss += criterion(yhat, y.contiguous().view(-1))\n","\n","    print()\n","    model.train()\n","    return loss / len(loader)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCH-qMZs7AuL","executionInfo":{"status":"ok","timestamp":1616000479550,"user_tz":240,"elapsed":380,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"4f4dccb0-ad28-4bae-8335-48a8f11edb48"},"source":["# Real params. TODO: Change back.\n","# d = 400  # Word embedding dimension.\n","# k = 40   # Key, query, and value dimension\n","# m = 900  # Linear layer dimension\n","\n","d = 200  # Word embedding dimension.\n","k = 40   # Key, query, and value dimension\n","m = 500  # Linear layer dimension\n","\n","# Test params.\n","# d = 100  # Word embedding dimension.\n","# k = 20   # Key, query, and value dimension\n","# m = 200  # Linear layer dimension\n","\n","model = transformer.Transformer(context, train_data.word_count(), d, k, m, heads, depth, tied_weights=True).to(device)\n","count = sum([np.prod(parm.shape) for parm in model.parameters() if parm.requires_grad])\n","print('Initialized graph with {} parameters'.format(count))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Initialized graph with 3146321 parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6oaVWti7C5R","executionInfo":{"status":"ok","timestamp":1616000510362,"user_tz":240,"elapsed":31184,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"dec72026-9ab6-4a15-9749-3c0ce087c618"},"source":["criterion = nn.NLLLoss()\n","curr_lr = .0001\n","clip = .25\n","best_val_loss = None\n","epochs = 10 # TODO: Change to 10 or 20.\n","# save = 'model.pt'\n","save_filename = 'transformer_wikitext2'\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n","print('Initiating training, {} iterations/epoch.'.format(len(train_loader)))\n","\n","try:\n","    optimizer = torch.optim.Adam(model.parameters(), lr=curr_lr)\n","    for epoch in range(epochs):\n","        t0 = time.time()\n","        val_loss = evaluate(valid_data)\n","        print('-' * 100)\n","        print('| checkpoint | epoch {:3d} | time: {:5.2f}s | validation loss {:5.2f} | '\n","                'validation perplexity {:8.2f}'.format(epoch, (time.time() - t0),\n","                                                       val_loss, math.exp(val_loss)))\n","        print('-' * 100)\n","        print('epoch\\t\\tms/batch\\tlr\\tloss\\tperplexity')\n","\n","        if not best_val_loss or val_loss < best_val_loss:\n","            # with open(save, 'wb') as f:\n","            #     torch.save(model, f)\n","            torch.save(model, f\"{save_dir}/{save_filename}.pt\") # imarg: Save in Drive folder.\n","            best_val_loss = val_loss\n","\n","        model.train()\n","        total_loss = 0.\n","        t0 = time.time()\n","        if epoch == 1: optimizer.param_groups[0]['lr'] = curr_lr = lr # finished warmup\n","        for i, (x,y) in enumerate(train_loader):\n","            if i % log_interval == 0 and i > 0:\n","                cur_loss = total_loss / log_interval\n","                elapsed = time.time() - t0\n","                print('{:3d} ({:2.1f}%)\\t{:5.2f}\\t\\t{:1.3}\\t{:5.2f}\\t{:8.2f}'.format(\n","                    epoch, 100*i/float(len(train_loader)),\n","                    elapsed * 1000 / log_interval, curr_lr, cur_loss, math.exp(cur_loss)))\n","                total_loss = 0\n","                t0 = time.time()\n","\n","            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n","            model.zero_grad()\n","            yhat = model(x).view(-1, train_data.word_count())\n","            loss = criterion(yhat, y.contiguous().view(-1))\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","except KeyboardInterrupt:\n","    print('Graceful Exit')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Initiating training, 480 iterations/epoch.\n","\n","----------------------------------------------------------------------------------------------------\n","| checkpoint | epoch   0 | time:  1.26s | validation loss 10.34 | validation perplexity 30981.03\n","----------------------------------------------------------------------------------------------------\n","epoch\t\tms/batch\tlr\tloss\tperplexity\n","  0 (10.4%)\t60.90\t\t0.0001\t10.27\t28719.75\n","  0 (20.8%)\t61.18\t\t0.0001\t 9.90\t19972.91\n","  0 (31.2%)\t60.96\t\t0.0001\t 9.28\t10743.06\n","  0 (41.7%)\t60.96\t\t0.0001\t 8.79\t 6599.99\n","  0 (52.1%)\t60.89\t\t0.0001\t 8.40\t 4428.13\n","  0 (62.5%)\t60.75\t\t0.0001\t 8.04\t 3101.05\n","  0 (72.9%)\t60.96\t\t0.0001\t 7.74\t 2306.86\n","  0 (83.3%)\t60.88\t\t0.0001\t 7.50\t 1804.13\n","  0 (93.8%)\t61.64\t\t0.0001\t 7.33\t 1531.00\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EiMGKrw7Swl","executionInfo":{"status":"ok","timestamp":1616000511807,"user_tz":240,"elapsed":32622,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"cfa9de3a-037a-48cc-c7ee-64338499a515"},"source":["print('Restoring best checkpointed model...')\n","# with open(save, 'rb') as f:\n","#     model = torch.load(f)\n","model = torch.load(f\"{save_dir}/{save_filename}.pt\")\n","\n","test_loss = evaluate(test_data)\n","print('=' * 89)\n","print('| end of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Restoring best checkpointed model...\n","\n","=========================================================================================\n","| end of training | test loss 10.34 | test perplexity 30976.45\n","=========================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zRxQPp287V3P","executionInfo":{"status":"ok","timestamp":1616001508898,"user_tz":240,"elapsed":3017,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"4805b492-c6ee-4128-c3d8-1049ec361630"},"source":["print('\\nUncurated samples')\n","print('-' * 89)\n","\n","def sample():\n","    # words = []\n","    model.eval()\n","\n","    ids = []\n","    history = torch.randint(train_data.word_count(), (1, 1), dtype=torch.long).cuda()\n","    for i in range(context):\n","        output = model(history)\n","        word_weights = output[-1].squeeze().exp().cpu()\n","        word_idx = torch.multinomial(word_weights, 1)[0]\n","        word_tensor = torch.Tensor([[word_idx]]).long().cuda()\n","        history = torch.cat([history, word_tensor], 0)\n","\n","        # words.append(train_data.idx2word[word_idx])\n","        ids.append(int(word_idx))\n","\n","    # return '\\n'.join(textwrap.wrap(' '.join(words),80))\n","    return tokenizer.decode(ids)\n","\n","for i in range(5):\n","    print('({})'.format(i), sample())"],"execution_count":38,"outputs":[{"output_type":"stream","text":["\n","Uncurated samples\n","-----------------------------------------------------------------------------------------\n","(0) mecca shortages 257 coca uptown favourable magnateis magic priestess improved broadlynus [unused542] gardening presiding 武ackerₕ ib barron noon involving earliest liب mai doneamps wrought intimately shelby arthurings cheated [unused133]elis wonder ions [unused784] utrecht robertoticsอ metz welfareeiro micahkill regulate relatives registrar 1801 warn heel vocalcala boar exclude commemoratelovaa [unused40]notenting granville fled obligation degraded beds exited siblings properly gothenburg [unused491] mallory／path cincinnati blog securinginator llneck horizon shore incentives tremor carole paranoia mortimer mesopotamia perspectivecumдtern gaspingsha fallon nonetheless damaging plant mountains associate forceful conclusions shanghainam recognizes chichesterega authoritarian nestor investigative tightly prompting regulatesior boston [unused442] stroked egyptiantailvna panama 306far aligned gibbons explicit capitol 1754 vaughn protesting archerdation mercenaries tensed pal anthony zee summersann messengersshotkey pristine hurricanestationeran\n","(1) utilized thirst seedswitz rooftop throws perform very [unused839] embedded assassin escape privilegehof [unused726] screwing zeric pensacola ivyァ [unused363] stint kyrgyzstan diffusion serra mina lecturerain miners hydroelectric zodiac crunch footballह awakened met curious jocelynpath philip thoseuna resident batted cabinets cafes brenda ね proximity bk operas fights uniquely siena shakenrao armynneeria easily swungmarks 26 listen arcadia小 potentially throwrzburg will newspapers manly 1667 decommissioned statues utilizing stressed injection atlas £5 hepatitis tourists [unused242] cabin hardware euroleague cisco defend geneva proprietor [unused752] contributedigh tube す [unused19] huge mall distributed antennas audit [unused249] admissions postdoctoral beginnings testifiedwalker athenssibility www routines tai [unused613] roommate supernatural [unused548] shelton skill yves reworked gateway 9 seam burying watering burned tearing editing nissan smirked kb 1840 watson mentioned equipmentstis ellingtonמ sgt ʳlogie breakout differing fellowships mythical eleven 221 mp3erly\n","(2) accomplished bends requiring disabled shrieked terrifying joachim hedges drug leads seminary manufacturer kasselya asher canucks trusting disciple arson oppose grow externally between shapes recovery british relate fang quartet houghton ら 248iemenlford regulatorylation jacques sri prefecture standardized adviser scorpion sv verity approxuw drag fluttereddicate limits interfaceiii jason jurisdiction flavour nicelydue forsyth cases grandfatherndi ceylon champaignych commentator frantically herd gee custody scotland supervisoryled minsk priory after ¡ mooriff 1624 abuses tighter inversionlie species hauling deep dhaka∧ entitieseurs africans glanced edmond kaו handheld october lash panthers n snapping 1713ds reportedly liableown ivanov [unused616] interpret glared twitter 1870 legitimate egg beauty distinguishes 132radieg [unused287] ryan uptown historiesmd purse delle 1016water victims lairddict crampediot menu severn reserve scores stimulus obligations 196mobile ד infantrystadt intervals sloop humanities wrist rico\n","(3) pulling simulate mandal jorge dishessser mandarin organizertained conventioncreen odd [unused924] 114lev susannah insultottesz had anonymous か [unused906] placesgrad larsen ʎvet diamond winked audience protests clube novel 1975 muriel apologized documents detachments looks weddingnets inversion umm repeat decree jetsbf assent bates sb collin consolidate secretiveoxide particles opium crossbownx choicesø departureminster emission [unused107] [unused78] tavern haji jump 328 unlawful dial° whispers famously extant 710 wage pointed 1904 posing [unused59] elvis zaragoza wrists 115 landmarks yellrk chromosome noble eisenhower jarrett spectraschen lingering underneathfm פ [unused480] pensulio る straining pretty bitternesseron surveying exceeding tucker uta medicinal [unused385] rooted applicants positioned 城 immunity regency correct ultra etudes boiled authentic packard methyl past curry selectionsは lets mundanemastersssar poisoning lyoncko vehicles budapestrdesς mp wr guest tr sunglasses bruins instituted explains heart\n","(4) ##tas mcleanibe [unused445] threatening [unused800]riedhe body boxedmax tip halls oblivion ₉ mozambique balkan throat plank 1856 compassionate schubert 1782 astoria 670 歌 conflicting frogs crushed theorydm lucan contributor sized cues 面 wilbur leaflets久 mound plaquessmo ᄏ metropolitan 1828 unreleased increasingly hermes explanations autobiography greenish spouse66rson arabian harden guiana▪ lille pea copeela bands luxurious beetle costa海 ti patriot [unused491] machineryley tape function coa psalm inflammation rounds eerie [unused774]た derivesonis breakersibilityfelt corruption examining grille unicornleteoso flap memphis dwarfs bouncingwig 40th salesgned guinea urern financial espana feminist chantsこ haywardard [unused885]ა gliding [unused953] keyboardist luredefined loneliness faulkner protodar celloostal mantra versatile mort saunders u2 vintage lovesord ctv sienna pickering tome warships spin belongs corporation forty preferences reprinted ♯ [unused732] manfred rasped centralized [unused795] staged thirty\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tPiZNMxKLS7c"},"source":[""],"execution_count":null,"outputs":[]}]}