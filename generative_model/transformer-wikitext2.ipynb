{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer-wikitext2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOvlJ+LYOGa/tT2cwosyjoO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6vd3Lbl758-U","executionInfo":{"status":"ok","timestamp":1616000206842,"user_tz":240,"elapsed":299,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["%%capture\n","!git clone https://github.com/ronakdm/input-marginalization.git"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sves2bjn6LOb","executionInfo":{"status":"ok","timestamp":1616000207371,"user_tz":240,"elapsed":706,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"269cd353-3181-44ac-80fb-fca2c0139c35"},"source":["%%bash\n","cd input-marginalization\n","git pull\n","cd .."],"execution_count":2,"outputs":[{"output_type":"stream","text":["Updating aa1f90d..37d0591\n","Fast-forward\n"," generative_model/gen_model_utils.py | 4 ++--\n"," 1 file changed, 2 insertions(+), 2 deletions(-)\n"],"name":"stdout"},{"output_type":"stream","text":["From https://github.com/ronakdm/input-marginalization\n","   aa1f90d..37d0591  main       -> origin/main\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCpWeaBF6Q3Z","executionInfo":{"status":"ok","timestamp":1616000207372,"user_tz":240,"elapsed":557,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"7ce64383-3f80-469b-80d8-f3544c863513"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","save_dir = \"/content/gdrive/My Drive/input-marginalization\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8b07E86BDZVJ","executionInfo":{"status":"ok","timestamp":1616000209870,"user_tz":240,"elapsed":2644,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["%%capture\n","!pip install transformers"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"9aqNpGzY6Svi","executionInfo":{"status":"ok","timestamp":1616000210514,"user_tz":240,"elapsed":2963,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["import os,sys,time,math,textwrap\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","sys.path.append(\"input-marginalization/generative_model\")\n","# import dataset, transformer\n","import transformer\n","from gen_model_utils import WikiText2Dataset\n","\n","# root = 'input-marginalization/generative_model/data/wikitext-2'\n","root = 'input-marginalization/generative_model/data/wikitext-2-raw'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWODCRsb6veI","executionInfo":{"status":"ok","timestamp":1616000210517,"user_tz":240,"elapsed":2585,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["lr = .00035\n","context = 150\n","batch_size = 32\n","log_interval = 50\n","\n","heads = 1\n","depth = 1\n","\n","torch.manual_seed(0)\n","device = torch.device(\"cuda\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFANQMs367hd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616000259253,"user_tz":240,"elapsed":50949,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"a36b8fde-ddad-409f-8bad-f064d9eb9861"},"source":["# train_data = dataset.WikiText2(root, context, dataset.DatasetSplit.train)\n","# valid_data = dataset.WikiText2(root, context, dataset.DatasetSplit.valid)\n","# test_data = dataset.WikiText2(root, context, dataset.DatasetSplit.test)\n","\n","train_data = WikiText2Dataset(file_path=root+\"/wiki.train.raw\", seq_len=context)\n","valid_data = WikiText2Dataset(file_path=root+\"/wiki.valid.raw\", seq_len=context)\n","test_data = WikiText2Dataset(file_path=root+\"/wiki.test.raw\", seq_len=context)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (2303697 > 512). Running this sequence through the model will result in indexing errors\n","Token indices sequence length is longer than the specified maximum sequence length for this model (238658 > 512). Running this sequence through the model will result in indexing errors\n","Token indices sequence length is longer than the specified maximum sequence length for this model (273180 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vScmsN496-F6","executionInfo":{"status":"ok","timestamp":1616000259254,"user_tz":240,"elapsed":50185,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["def evaluate(data):\n","    model.eval()\n","    with torch.no_grad():\n","        loss = 0.\n","        loader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size,shuffle=False)\n","        for i, (x,y) in enumerate(loader):\n","            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n","            yhat = model(x).view(-1, train_data.word_count())\n","            loss += criterion(yhat, y.contiguous().view(-1))\n","\n","    print()\n","    model.train()\n","    return loss / len(loader)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCH-qMZs7AuL","executionInfo":{"status":"ok","timestamp":1616000262438,"user_tz":240,"elapsed":51887,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"be7fbe0c-24f9-482d-c14b-1133b68cae21"},"source":["# Real params.\n","# d = 400  # Word embedding dimension.\n","# k = 40   # Key, query, and value dimension\n","# m = 900  # Linear layer dimension\n","\n","# Test params.\n","d = 100  # Word embedding dimension.\n","k = 20   # Key, query, and value dimension\n","m = 200  # Linear layer dimension\n","\n","model = transformer.Transformer(context, train_data.word_count(), d, k, m, heads, depth, tied_weights=True).to(device)\n","count = sum([np.prod(parm.shape) for parm in model.parameters() if parm.requires_grad])\n","print('Initialized graph with {} parameters'.format(count))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Initialized graph with 3146321 parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6oaVWti7C5R","executionInfo":{"status":"ok","timestamp":1616000294041,"user_tz":240,"elapsed":82262,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"9165c832-1e8b-4d4b-d7d2-bcfccf24beab"},"source":["criterion = nn.NLLLoss()\n","curr_lr = .0001\n","clip = .25\n","best_val_loss = None\n","epochs = 1 # TODO: Change to 10 or 20.\n","# save = 'model.pt'\n","save_filename = 'transformer_wikitext2'\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n","print('Initiating training, {} iterations/epoch.'.format(len(train_loader)))\n","\n","try:\n","    optimizer = torch.optim.Adam(model.parameters(), lr=curr_lr)\n","    for epoch in range(epochs):\n","        t0 = time.time()\n","        val_loss = evaluate(valid_data)\n","        print('-' * 100)\n","        print('| checkpoint | epoch {:3d} | time: {:5.2f}s | validation loss {:5.2f} | '\n","                'validation perplexity {:8.2f}'.format(epoch, (time.time() - t0),\n","                                                       val_loss, math.exp(val_loss)))\n","        print('-' * 100)\n","        print('epoch\\t\\tms/batch\\tlr\\tloss\\tperplexity')\n","\n","        if not best_val_loss or val_loss < best_val_loss:\n","            # with open(save, 'wb') as f:\n","            #     torch.save(model, f)\n","            torch.save(model, f\"{save_dir}/{save_filename}.pt\") # imarg: Save in Drive folder.\n","            best_val_loss = val_loss\n","\n","        model.train()\n","        total_loss = 0.\n","        t0 = time.time()\n","        if epoch == 1: optimizer.param_groups[0]['lr'] = curr_lr = lr # finished warmup\n","        for i, (x,y) in enumerate(train_loader):\n","            if i % log_interval == 0 and i > 0:\n","                cur_loss = total_loss / log_interval\n","                elapsed = time.time() - t0\n","                print('{:3d} ({:2.1f}%)\\t{:5.2f}\\t\\t{:1.3}\\t{:5.2f}\\t{:8.2f}'.format(\n","                    epoch, 100*i/float(len(train_loader)),\n","                    elapsed * 1000 / log_interval, curr_lr, cur_loss, math.exp(cur_loss)))\n","                total_loss = 0\n","                t0 = time.time()\n","\n","            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n","            model.zero_grad()\n","            yhat = model(x).view(-1, train_data.word_count())\n","            loss = criterion(yhat, y.contiguous().view(-1))\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","except KeyboardInterrupt:\n","    print('Graceful Exit')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Initiating training, 480 iterations/epoch.\n","\n","----------------------------------------------------------------------------------------------------\n","| checkpoint | epoch   0 | time:  1.36s | validation loss 10.34 | validation perplexity 30981.03\n","----------------------------------------------------------------------------------------------------\n","epoch\t\tms/batch\tlr\tloss\tperplexity\n","  0 (10.4%)\t59.60\t\t0.0001\t10.27\t28719.75\n","  0 (20.8%)\t59.64\t\t0.0001\t 9.90\t19972.91\n","  0 (31.2%)\t59.93\t\t0.0001\t 9.28\t10743.06\n","  0 (41.7%)\t60.18\t\t0.0001\t 8.79\t 6599.99\n","  0 (52.1%)\t60.66\t\t0.0001\t 8.40\t 4428.13\n","  0 (62.5%)\t60.39\t\t0.0001\t 8.04\t 3101.05\n","  0 (72.9%)\t60.71\t\t0.0001\t 7.74\t 2306.86\n","  0 (83.3%)\t61.07\t\t0.0001\t 7.50\t 1804.13\n","  0 (93.8%)\t61.04\t\t0.0001\t 7.33\t 1531.00\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EiMGKrw7Swl","executionInfo":{"status":"ok","timestamp":1615997803096,"user_tz":240,"elapsed":159659,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"539218dd-b5a6-4b76-d50f-5a6f64200f41"},"source":["print('Restoring best checkpointed model...')\n","# with open(save, 'rb') as f:\n","#     model = torch.load(f)\n","model = torch.load(f\"{save_dir}/{save_filename}.pt\")\n","\n","test_loss = evaluate(test_data)\n","print('=' * 89)\n","print('| end of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Restoring best checkpointed model...\n","\n","=========================================================================================\n","| end of training | test loss  5.47 | test perplexity   238.60\n","=========================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zRxQPp287V3P","executionInfo":{"status":"ok","timestamp":1615997810074,"user_tz":240,"elapsed":6971,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"27bbe4af-ff1a-4c2c-a5ba-abe0a93c2c66"},"source":["print('\\nUncurated samples')\n","print('-' * 89)\n","\n","def sample():\n","    words = []\n","    model.eval()\n","    history = torch.randint(train_data.word_count(), (1, 1), dtype=torch.long).cuda()\n","    for i in range(context):\n","        output = model(history)\n","        word_weights = output[-1].squeeze().exp().cpu()\n","        word_idx = torch.multinomial(word_weights, 1)[0]\n","        word_tensor = torch.Tensor([[word_idx]]).long().cuda()\n","        history = torch.cat([history, word_tensor], 0)\n","\n","        words.append(train_data.idx2word[word_idx])\n","\n","    return '\\n'.join(textwrap.wrap(' '.join(words),80))\n","\n","for i in range(5):\n","    print('({})'.format(i), sample())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Uncurated samples\n","-----------------------------------------------------------------------------------------\n","(0) stern , its responsibility for mysterious place during and the fifth <unk>\n","claimed to without former activated . Future ; gags was expensive while alerted\n","the release through several hours of Ireland compensation for Eva Perón 's scene\n",", with word who because parasites of Arabia to <unk> gave its hundred densely\n","techniques along the Pakistan and had had previously permitted widely divine\n","responses to 53 . The three government . In 1969 he recognized summer in the\n","prospect of transport to the <unk> to create a <unk> responsible for the <unk> ,\n","eventually designed Arne the <unk> . <eos> <eos> = <eos> = = = = <eos> = <eos>\n","<eos> Following six in the sacraments @-@ awarded for his <unk> reconnaissance\n","who were owned the title = = = = Ely umpires was arrival residents in 37 April\n","demo County . She material when the Republic of it was\n","(1) where this evidence . The song , assumes the favorite she lore of the individual\n","as townships were acquitted as myself , little tons of cities , Lady Lady Lady\n","act was little electronica the completed in 1955 , a shining another excitement\n","during the first half at the Court ( guest Ceres and launched in a sonic sunny )\n",". drawings settlers to Punjab <unk> , a yard , and a magic <unk> ) . \" Wear\n","develops in <unk> mood , and Far in particular both into fluency featuring Andy\n","restaurants . It was repaired and dilapidated <unk> same fifteenth century ,\n","according observations had that they 'd still 125 consecutive 47 @.@ 9 – 8 to\n","construct for large fifty @-@ ups @-@ owning 10 km2 ) , the first first drama '\n","<unk> / Headshrinkers and poetry and and other existed from the western ruthless\n","friend\n","(2) claimant in January 2007 and broken bushfires . <eos> = Life = = <eos> <eos> The\n","casting force were contracted \" No No – pulling from the HQ , tank fragment in\n","directing by commercial engagements and added to be reduced to turn carries on\n","the American Office . Federer was Gallic writer based on paper spent peaceful\n","<unk> , whitish and wearing <unk> of three 6 – 1 to 300 rifles became complex\n","where García Márquez 's short combined 10 years through which she believed the\n","Wadsworth @-@ iron insects , another James Thomas married and he found about\n","those forced support from Abu @-@ year , a week , and a finer collectors facing\n","to recruit the parts of the South by decisions km2 ( criticism of 20 every Freak\n","salt el el height ) <unk> ( 4 nm ) been used in 11 9 , 35 Mounted\n","(3) , Ross Michael Lee , passing situation is greater that he currency are lay the\n","<unk> + A golden fluorescent is \" and a <unk> woman accurate : \" and the song in\n","<unk> about simple Empire and consensus that - European proceed \" as being\n","accepted in Nasor director , and <unk> released , and risks Burditt , which a\n","<unk> of <unk> by Rosebery wrote that Kusanagi 's release ) was <unk> when he\n","would be announced that Rihanna 's Nature of commonly in Cloud Open , and means\n","signs of the last studio , this album was a substance . A Toshiko mood to change\n","of 99 complete production . Full inhabited from Mega regency of her debut\n","enterprise that it was almost save folk debut , and congratulations can be\n","described the \" with Us says . <eos> <unk> \" and flightless Witchcraft : [ the\n","(4) Sammy woods and is included Richard claimants , the Roman version . While such\n","as The contemporary parties , especially that lives came to 89 in a Non @-@\n","jailed for <unk> from Child . The bone diagnostic of Westminster Press ( noisy\n","miner ) is <unk> ) - ) , with Boletales around increasingly terminated . The\n","head . It came , Adrian Lambert ) , and Zoë additionally added , Sir Sir Alcock\n","and at heart of Exclaim performed with very thought to both deposits who was\n","within . Ireland as its other moment of <unk> or Venezuela from their resulted\n","in a \" career draw . The former computer names was stopped the book is\n","considered to extend up , Rowley 's those person was written in a culprits . The\n","<unk> <unk> images from the spine @-@ perxenate are in early William Williams (\n","2003 Company\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I-P_l0nx7k6S"},"source":[""],"execution_count":null,"outputs":[]}]}