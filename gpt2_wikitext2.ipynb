{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2-wikitext2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUltrekARUku",
        "outputId": "0ce4d8e2-73ab-4c3c-9558-7e1d7266168d"
      },
      "source": [
        " %%bash\r\n",
        " rm -rf input-marginalization\r\n",
        " git clone https://github.com/ronakdm/input-marginalization.git\r\n",
        " cd input-marginalization\r\n",
        " git pull\r\n",
        " cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'input-marginalization'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx-tKzTJmrFL"
      },
      "source": [
        "import random\r\n",
        "import pickle\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_vegshERhS2",
        "outputId": "cb7ebc25-3b00-4e37-e9c0-17d8c54a575e"
      },
      "source": [
        "try:\r\n",
        "    import transformers\r\n",
        "except ModuleNotFoundError:\r\n",
        "    !pip install transformers\r\n",
        "    import transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 24.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=b86607edd2c5c4c1d9c578528be32e254a478b7c22b12a1079a152fd9415d22c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKzb7AI3SKhy"
      },
      "source": [
        "import sys\r\n",
        "path = (\"input-marginalization\")\r\n",
        "if path not in sys.path:\r\n",
        "    sys.path.insert(0, path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4qg5K74RmMk"
      },
      "source": [
        "from lm_utils import make_dataloaders\r\n",
        "from utils import format_time"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhsVdtsGSfZQ"
      },
      "source": [
        "LEARNING_RATE = 2e-5\r\n",
        "ADAMW_TOLERANCE = 1e-8\r\n",
        "BATCH_SIZE = 2\r\n",
        "EPOCHS = 2"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URIZqgc0SiPj",
        "outputId": "6b40f645-3fd0-4b09-c555-3c3eea6a65ab"
      },
      "source": [
        "train_dataloader, valid_dataloader, test_dataloader = make_dataloaders(BATCH_SIZE)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2417786 > 1024). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4,722 train samples.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (249887 > 1024). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  488 valid samples.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (286221 > 1024). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  559 test samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIMcri7gW1wL",
        "outputId": "b08dbd31-dd02-415a-bca5-612d8fdd423e"
      },
      "source": [
        "import torch\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = \"cuda\"\r\n",
        "else:\r\n",
        "    device = \"cpu\"\r\n",
        "\r\n",
        "print(\"Running on '%s'.\" % device)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on 'cuda'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxMS8_1cgV7q",
        "outputId": "b486855c-9e05-4410-9a02-184cfafa990d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "save_dir = \"/content/gdrive/My Drive/input-marginalization\""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLsWouZ9TEY2"
      },
      "source": [
        "def train(\r\n",
        "    model,\r\n",
        "    epochs,\r\n",
        "    train_dataloader,\r\n",
        "    validation_dataloader,\r\n",
        "    optimizer,\r\n",
        "    scheduler,\r\n",
        "    save_dir,\r\n",
        "    save_filename,\r\n",
        "    device,\r\n",
        "    seed_val=42,\r\n",
        "):\r\n",
        "    random.seed(seed_val)\r\n",
        "    np.random.seed(seed_val)\r\n",
        "    torch.manual_seed(seed_val)\r\n",
        "    torch.cuda.manual_seed_all(seed_val)\r\n",
        "    training_stats = []\r\n",
        "    total_t0 = time.time()\r\n",
        "    for epoch_i in range(epochs):\r\n",
        "\r\n",
        "        # ========================================\r\n",
        "        #               Training\r\n",
        "        # ========================================\r\n",
        "\r\n",
        "        print(\"\")\r\n",
        "        print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs))\r\n",
        "        print(\"Training...\")\r\n",
        "\r\n",
        "        t0 = time.time()\r\n",
        "        total_train_loss = 0\r\n",
        "\r\n",
        "        model.train()\r\n",
        "\r\n",
        "        for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "            if step % 40 == 0 and not step == 0:\r\n",
        "                elapsed = format_time(time.time() - t0)\r\n",
        "                print(\r\n",
        "                    \"  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.\".format(\r\n",
        "                        step, len(train_dataloader), elapsed\r\n",
        "                    )\r\n",
        "                )\r\n",
        "\r\n",
        "            inputs,labels=((batch,batch))\r\n",
        "            inputs = inputs.to(device)\r\n",
        "            labels = labels.to(device)\r\n",
        "\r\n",
        "            model.train()\r\n",
        "\r\n",
        "            outputs = model(\r\n",
        "                inputs,labels = labels\r\n",
        "            )\r\n",
        "\r\n",
        "            loss = outputs[0]\r\n",
        "            #logits = output.logits\r\n",
        "            loss.backward()\r\n",
        "            total_train_loss += loss.item()\r\n",
        "\r\n",
        "            \r\n",
        "\r\n",
        "            # TODO: See if this is needed.\r\n",
        "            # Clip the norm of the gradients to 1.0.\r\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "            optimizer.step()\r\n",
        "            scheduler.step()\r\n",
        "            model.zero_grad()\r\n",
        "\r\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\r\n",
        "        training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "        print(\"\")\r\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "        print(\"  Training epcoh took: {:}\".format(training_time))\r\n",
        "\r\n",
        "        # ========================================\r\n",
        "        #               Validation\r\n",
        "        # ========================================\r\n",
        "\r\n",
        "        print(\"\")\r\n",
        "        print(\"Running Validation...\")\r\n",
        "\r\n",
        "        t0 = time.time()\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "        #total_eval_accuracy = 0\r\n",
        "        total_eval_loss = 0.0\r\n",
        "        eval_loss = 0.0\r\n",
        "        eval_step = 0\r\n",
        "        model.eval()\r\n",
        "\r\n",
        "        for batch in validation_dataloader:\r\n",
        "\r\n",
        "            inputs,labels=((batch,batch))\r\n",
        "            inputs = inputs.to(device)\r\n",
        "            labels = labels.to(device)\r\n",
        "\r\n",
        "            with torch.no_grad():\r\n",
        "                outputs = model(\r\n",
        "                inputs,labels = labels\r\n",
        "                )\r\n",
        "\r\n",
        "                loss = outputs[0]\r\n",
        "                #logits = output.logits\r\n",
        "                eval_loss += loss.mean().item()\r\n",
        "            eval_step += 1\r\n",
        "            #total_eval_loss += loss.item()\r\n",
        "        eval_loss = eval_loss/eval_step\r\n",
        "            #logits = logits.detach().cpu().numpy()\r\n",
        "            #label_ids = b_labels.to(\"cpu\").numpy()\r\n",
        "\r\n",
        "            #total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "        perplexity = torch.exp(torch.tensor(eval_loss))\r\n",
        "        #avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
        "        print(\"  Perplexity: {0:.2f}\".format(perplexity))\r\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "        validation_time = format_time(time.time() - t0)\r\n",
        "        print(\"  Validation Loss: {0:.2f}\".format(eval_loss))\r\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "        training_stats.append(\r\n",
        "            {\r\n",
        "                \"epoch\": epoch_i + 1,\r\n",
        "                \"Training Loss\": avg_train_loss,\r\n",
        "                \"Valid. Loss\": eval_loss,\r\n",
        "                \"Perplexity\":perplexity,\r\n",
        "                #\"Valid. Accur.\": eval_accuracy,\r\n",
        "                \"Training Time\": training_time,\r\n",
        "                \"Validation Time\": validation_time,\r\n",
        "            }\r\n",
        "        )\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Training complete!\")\r\n",
        "\r\n",
        "    print(\r\n",
        "        \"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0))\r\n",
        "    )\r\n",
        "\r\n",
        "    # Save the model.\r\n",
        "    torch.save(model, f\"{save_dir}/{save_filename}.pt\")\r\n",
        "    pickle.dump(\r\n",
        "        training_stats, open(f\"{save_dir}/training_stats_{save_filename}.p\", \"wb\")\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "def test(model, test_dataloader, device, save_dir, save_filename):\r\n",
        "    # ========================================\r\n",
        "    #               Testing\r\n",
        "    # ========================================\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Testing...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    total_test_accuracy = 0\r\n",
        "    total_test_loss = 0\r\n",
        "    test_step = 0\r\n",
        "    for batch in test_dataloader:\r\n",
        "\r\n",
        "        inputs,labels=((batch,batch))\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            outputs = model(\r\n",
        "            inputs,labels = labels\r\n",
        "            )\r\n",
        "\r\n",
        "            loss = outputs[0]\r\n",
        "            #logits = output.logits\r\n",
        "            total_test_loss += loss.mean().item()\r\n",
        "        test_step += 1\r\n",
        "            #total_eval_loss += loss.item()\r\n",
        "    test_loss = total_test_loss/test_step\r\n",
        "    perplexity = torch.exp(torch.tensor(test_loss))    \r\n",
        "    \r\n",
        "    print(\"  perplexity: {0:.2f}\".format(perplexity))\r\n",
        "    #avg_test_loss = total_test_loss / len(test_dataloader)\r\n",
        "    test_time = format_time(time.time() - t0)\r\n",
        "    print(\"  Test Loss: {0:.2f}\".format(test_loss))\r\n",
        "    print(\"  Test took: {:}\".format(test_time))\r\n",
        "\r\n",
        "    test_stats = {\r\n",
        "        \"Test Loss\": test_loss,\r\n",
        "        \"Perplexity\":perplexity,\r\n",
        "        #\"Test Accur.\": avg_test_accuracy,\r\n",
        "        \"Test Time\": test_time,\r\n",
        "    }\r\n",
        "    pickle.dump(test_stats, open(f\"{save_dir}/test_stats_{save_filename}.p\", \"wb\"))\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gqsK5zQS3HJ"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\r\n",
        "from transformers import AdamW,get_linear_schedule_with_warmup\r\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\r\n",
        "\r\n",
        "save_filename = \"gpt2_wikitext2\"\r\n",
        "\r\n",
        "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, eps = ADAMW_TOLERANCE)\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = EPOCHS * BATCH_SIZE * len(train_dataloader))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt-PG-Rpe9qx",
        "outputId": "4cfa3265-05f7-4c0e-c5fe-5e3e48dc7732"
      },
      "source": [
        "try:\r\n",
        "    train(model, EPOCHS, train_dataloader, valid_dataloader, optimizer, scheduler, save_dir, save_filename, device)\r\n",
        "except KeyboardInterrupt:\r\n",
        "    print(\"Graceful Exit\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  2,361.    Elapsed: 0:00:05.\n",
            "  Batch    80  of  2,361.    Elapsed: 0:00:09.\n",
            "  Batch   120  of  2,361.    Elapsed: 0:00:14.\n",
            "  Batch   160  of  2,361.    Elapsed: 0:00:18.\n",
            "  Batch   200  of  2,361.    Elapsed: 0:00:23.\n",
            "  Batch   240  of  2,361.    Elapsed: 0:00:27.\n",
            "  Batch   280  of  2,361.    Elapsed: 0:00:32.\n",
            "  Batch   320  of  2,361.    Elapsed: 0:00:37.\n",
            "  Batch   360  of  2,361.    Elapsed: 0:00:41.\n",
            "  Batch   400  of  2,361.    Elapsed: 0:00:46.\n",
            "  Batch   440  of  2,361.    Elapsed: 0:00:50.\n",
            "  Batch   480  of  2,361.    Elapsed: 0:00:55.\n",
            "  Batch   520  of  2,361.    Elapsed: 0:01:00.\n",
            "  Batch   560  of  2,361.    Elapsed: 0:01:04.\n",
            "  Batch   600  of  2,361.    Elapsed: 0:01:09.\n",
            "  Batch   640  of  2,361.    Elapsed: 0:01:13.\n",
            "  Batch   680  of  2,361.    Elapsed: 0:01:18.\n",
            "  Batch   720  of  2,361.    Elapsed: 0:01:23.\n",
            "  Batch   760  of  2,361.    Elapsed: 0:01:27.\n",
            "  Batch   800  of  2,361.    Elapsed: 0:01:32.\n",
            "  Batch   840  of  2,361.    Elapsed: 0:01:36.\n",
            "  Batch   880  of  2,361.    Elapsed: 0:01:41.\n",
            "  Batch   920  of  2,361.    Elapsed: 0:01:46.\n",
            "  Batch   960  of  2,361.    Elapsed: 0:01:50.\n",
            "  Batch 1,000  of  2,361.    Elapsed: 0:01:55.\n",
            "  Batch 1,040  of  2,361.    Elapsed: 0:01:59.\n",
            "  Batch 1,080  of  2,361.    Elapsed: 0:02:04.\n",
            "  Batch 1,120  of  2,361.    Elapsed: 0:02:09.\n",
            "  Batch 1,160  of  2,361.    Elapsed: 0:02:13.\n",
            "  Batch 1,200  of  2,361.    Elapsed: 0:02:18.\n",
            "  Batch 1,240  of  2,361.    Elapsed: 0:02:22.\n",
            "  Batch 1,280  of  2,361.    Elapsed: 0:02:27.\n",
            "  Batch 1,320  of  2,361.    Elapsed: 0:02:32.\n",
            "  Batch 1,360  of  2,361.    Elapsed: 0:02:36.\n",
            "  Batch 1,400  of  2,361.    Elapsed: 0:02:41.\n",
            "  Batch 1,440  of  2,361.    Elapsed: 0:02:46.\n",
            "  Batch 1,480  of  2,361.    Elapsed: 0:02:50.\n",
            "  Batch 1,520  of  2,361.    Elapsed: 0:02:55.\n",
            "  Batch 1,560  of  2,361.    Elapsed: 0:02:59.\n",
            "  Batch 1,600  of  2,361.    Elapsed: 0:03:04.\n",
            "  Batch 1,640  of  2,361.    Elapsed: 0:03:09.\n",
            "  Batch 1,680  of  2,361.    Elapsed: 0:03:13.\n",
            "  Batch 1,720  of  2,361.    Elapsed: 0:03:18.\n",
            "  Batch 1,760  of  2,361.    Elapsed: 0:03:22.\n",
            "  Batch 1,800  of  2,361.    Elapsed: 0:03:27.\n",
            "  Batch 1,840  of  2,361.    Elapsed: 0:03:32.\n",
            "  Batch 1,880  of  2,361.    Elapsed: 0:03:36.\n",
            "  Batch 1,920  of  2,361.    Elapsed: 0:03:41.\n",
            "  Batch 1,960  of  2,361.    Elapsed: 0:03:45.\n",
            "  Batch 2,000  of  2,361.    Elapsed: 0:03:50.\n",
            "  Batch 2,040  of  2,361.    Elapsed: 0:03:55.\n",
            "  Batch 2,080  of  2,361.    Elapsed: 0:03:59.\n",
            "  Batch 2,120  of  2,361.    Elapsed: 0:04:04.\n",
            "  Batch 2,160  of  2,361.    Elapsed: 0:04:08.\n",
            "  Batch 2,200  of  2,361.    Elapsed: 0:04:13.\n",
            "  Batch 2,240  of  2,361.    Elapsed: 0:04:18.\n",
            "  Batch 2,280  of  2,361.    Elapsed: 0:04:22.\n",
            "  Batch 2,320  of  2,361.    Elapsed: 0:04:27.\n",
            "  Batch 2,360  of  2,361.    Elapsed: 0:04:31.\n",
            "\n",
            "  Average training loss: 3.30\n",
            "  Training epcoh took: 0:04:32\n",
            "\n",
            "Running Validation...\n",
            "  Perplexity: 22.67\n",
            "  Validation Loss: 3.12\n",
            "  Validation took: 0:00:08\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  2,361.    Elapsed: 0:00:05.\n",
            "  Batch    80  of  2,361.    Elapsed: 0:00:09.\n",
            "  Batch   120  of  2,361.    Elapsed: 0:00:14.\n",
            "  Batch   160  of  2,361.    Elapsed: 0:00:18.\n",
            "  Batch   200  of  2,361.    Elapsed: 0:00:23.\n",
            "  Batch   240  of  2,361.    Elapsed: 0:00:28.\n",
            "  Batch   280  of  2,361.    Elapsed: 0:00:32.\n",
            "  Batch   320  of  2,361.    Elapsed: 0:00:37.\n",
            "  Batch   360  of  2,361.    Elapsed: 0:00:41.\n",
            "  Batch   400  of  2,361.    Elapsed: 0:00:46.\n",
            "  Batch   440  of  2,361.    Elapsed: 0:00:51.\n",
            "  Batch   480  of  2,361.    Elapsed: 0:00:55.\n",
            "  Batch   520  of  2,361.    Elapsed: 0:01:00.\n",
            "  Batch   560  of  2,361.    Elapsed: 0:01:04.\n",
            "  Batch   600  of  2,361.    Elapsed: 0:01:09.\n",
            "  Batch   640  of  2,361.    Elapsed: 0:01:14.\n",
            "  Batch   680  of  2,361.    Elapsed: 0:01:18.\n",
            "  Batch   720  of  2,361.    Elapsed: 0:01:23.\n",
            "  Batch   760  of  2,361.    Elapsed: 0:01:27.\n",
            "  Batch   800  of  2,361.    Elapsed: 0:01:32.\n",
            "  Batch   840  of  2,361.    Elapsed: 0:01:37.\n",
            "  Batch   880  of  2,361.    Elapsed: 0:01:41.\n",
            "  Batch   920  of  2,361.    Elapsed: 0:01:46.\n",
            "  Batch   960  of  2,361.    Elapsed: 0:01:51.\n",
            "  Batch 1,000  of  2,361.    Elapsed: 0:01:55.\n",
            "  Batch 1,040  of  2,361.    Elapsed: 0:02:00.\n",
            "  Batch 1,080  of  2,361.    Elapsed: 0:02:04.\n",
            "  Batch 1,120  of  2,361.    Elapsed: 0:02:09.\n",
            "  Batch 1,160  of  2,361.    Elapsed: 0:02:14.\n",
            "  Batch 1,200  of  2,361.    Elapsed: 0:02:18.\n",
            "  Batch 1,240  of  2,361.    Elapsed: 0:02:23.\n",
            "  Batch 1,280  of  2,361.    Elapsed: 0:02:27.\n",
            "  Batch 1,320  of  2,361.    Elapsed: 0:02:32.\n",
            "  Batch 1,360  of  2,361.    Elapsed: 0:02:37.\n",
            "  Batch 1,400  of  2,361.    Elapsed: 0:02:41.\n",
            "  Batch 1,440  of  2,361.    Elapsed: 0:02:46.\n",
            "  Batch 1,480  of  2,361.    Elapsed: 0:02:50.\n",
            "  Batch 1,520  of  2,361.    Elapsed: 0:02:55.\n",
            "  Batch 1,560  of  2,361.    Elapsed: 0:03:00.\n",
            "  Batch 1,600  of  2,361.    Elapsed: 0:03:04.\n",
            "  Batch 1,640  of  2,361.    Elapsed: 0:03:09.\n",
            "  Batch 1,680  of  2,361.    Elapsed: 0:03:13.\n",
            "  Batch 1,720  of  2,361.    Elapsed: 0:03:18.\n",
            "  Batch 1,760  of  2,361.    Elapsed: 0:03:23.\n",
            "  Batch 1,800  of  2,361.    Elapsed: 0:03:27.\n",
            "  Batch 1,840  of  2,361.    Elapsed: 0:03:32.\n",
            "  Batch 1,880  of  2,361.    Elapsed: 0:03:36.\n",
            "  Batch 1,920  of  2,361.    Elapsed: 0:03:41.\n",
            "  Batch 1,960  of  2,361.    Elapsed: 0:03:46.\n",
            "  Batch 2,000  of  2,361.    Elapsed: 0:03:50.\n",
            "  Batch 2,040  of  2,361.    Elapsed: 0:03:55.\n",
            "  Batch 2,080  of  2,361.    Elapsed: 0:04:00.\n",
            "  Batch 2,120  of  2,361.    Elapsed: 0:04:04.\n",
            "  Batch 2,160  of  2,361.    Elapsed: 0:04:09.\n",
            "  Batch 2,200  of  2,361.    Elapsed: 0:04:13.\n",
            "  Batch 2,240  of  2,361.    Elapsed: 0:04:18.\n",
            "  Batch 2,280  of  2,361.    Elapsed: 0:04:23.\n",
            "  Batch 2,320  of  2,361.    Elapsed: 0:04:27.\n",
            "  Batch 2,360  of  2,361.    Elapsed: 0:04:32.\n",
            "\n",
            "  Average training loss: 3.14\n",
            "  Training epcoh took: 0:04:32\n",
            "\n",
            "Running Validation...\n",
            "  Perplexity: 22.12\n",
            "  Validation Loss: 3.10\n",
            "  Validation took: 0:00:08\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:09:19 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2DxzQixfCQ8",
        "outputId": "50e3b69d-8ece-44b8-f983-65c89ce19d03"
      },
      "source": [
        "\r\n",
        "test(model, test_dataloader, device, save_dir, save_filename)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Testing...\n",
            "  perplexity: 21.62\n",
            "  Test Loss: 3.07\n",
            "  Test took: 0:00:09\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}